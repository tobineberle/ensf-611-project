{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSF611 Project\n",
    "**Authors: Tobin Eberle, Tom Wilson, Jeff Wheeler**\n",
    "\n",
    "### Project Outline\n",
    "\n",
    "Avalanche forecasting is the practice of observing current and historical snowpack conditions to make educated guesses about how reactive the snowpack is, and thus the likelihood of avalanches. It is a mixture of an art and a science where field observations and a forecaster's experience generally have a lot of weight in the final avalanche rating for a particular area. This project will investigate the classification of avalanche ratings for each of the three avalanche zones in Yoho National Park (Canada), to determine how accurately we can determine the likelihood of avalanches solely based on weather data.\n",
    "\n",
    "For reference the avalanche danger ratings are as follows:\n",
    "- No Rating: Summer, or no snow in the mountains.\n",
    "- Spring Conditions: Very variable avalanche conditions based on the fluctuation of temperature throughout the day making it difficult to forecast.\n",
    "- Low: Generally safe avalanche conditions.\n",
    "- Moderate: Heightened avalanche conditions.\n",
    "- Considerable: Dangerous avalanche conditions.\n",
    "- High: Very dangerous avalanche conditions.\n",
    "- Exterme: Extraordinarily dangerous avalanche conditions.\n",
    "\n",
    "The avalanche zones are split into three seperate areas:\n",
    "- Below Treeline (btl): Elevation of a mountain band that is covered by forest.\n",
    "- Treeline (tln): Sparse forest cover and is the transition between uniform forest cover below it and the alpine above it.\n",
    "- Alpine (alp): Wide expanses of open, exposed terrain with few or no trees.\n",
    "\n",
    "It is important to note there is a  difference between the avalanche zones as they each obtain a seperate rating from eachother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "Pre-process the data to remove NaN's and fill missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2140 entries, 0 to 2139\n",
      "Data columns (total 29 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   date                     2140 non-null   object \n",
      " 1   btl_rating               2140 non-null   object \n",
      " 2   tln_rating               2140 non-null   object \n",
      " 3   alp_rating               2140 non-null   object \n",
      " 4   problem_1                880 non-null    object \n",
      " 5   problem_2                565 non-null    object \n",
      " 6   problem_3                157 non-null    object \n",
      " 7   chance_1                 880 non-null    object \n",
      " 8   chance_2                 565 non-null    object \n",
      " 9   chance_3                 157 non-null    object \n",
      " 10  longitude                2140 non-null   float64\n",
      " 11  latitude                 2140 non-null   float64\n",
      " 12  station_name             2140 non-null   object \n",
      " 13  climate_id               2140 non-null   object \n",
      " 14  year                     2140 non-null   int64  \n",
      " 15  month                    2140 non-null   int64  \n",
      " 16  day                      2140 non-null   int64  \n",
      " 17  data_quality             0 non-null      float64\n",
      " 18  max_temp_c               1839 non-null   float64\n",
      " 19  min_temp_c               1838 non-null   float64\n",
      " 20  mean_temp_c              1838 non-null   float64\n",
      " 21  heat_deg_day_c           1838 non-null   float64\n",
      " 22  cool_deg_day_c           1838 non-null   float64\n",
      " 23  total_rain_mm            0 non-null      float64\n",
      " 24  total_snow_cm            0 non-null      float64\n",
      " 25  total_precip_mm          1420 non-null   float64\n",
      " 26  snow_on_ground_cm        1707 non-null   float64\n",
      " 27  dir_of_max_gust_10s_deg  1495 non-null   float64\n",
      " 28  spd_of_max_gust_kmh      1495 non-null   float64\n",
      "dtypes: float64(14), int64(3), object(12)\n",
      "memory usage: 485.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#ignoring some deprication warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "#Import the dataset and inspect the contents\n",
    "import pandas as pd\n",
    "df = pd.read_csv('av_dataset_ensf611.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy to deal with missing values in the following columns is as follows:\n",
    "\n",
    "```date```: Drop first date column as it contains redundant information.\n",
    "\n",
    "```btl/tln/alp_rating```: Delete NaN rows.\n",
    "\n",
    "```problem_1/2/3```: Replace NaN with 'noproblem'. Format column to ensure no '-' seperates the problem description.\n",
    "\n",
    "```chance_1/2/3```: Drop these columns as they are determined by forecasters and will skew results.\n",
    "\n",
    "```longitude/latitude/climate_id/data_quality/station_name```: Drop columns as not relevant to dataset.\n",
    "\n",
    "```year/month/day```: No missing values\n",
    "\n",
    "```max/min/mean_temp```: Forward fill (extreme of temps should be relatively simlar day-to-day)\n",
    "\n",
    "```hot/cool_deg_days```: Forward fill. Otherwise could drop these columns as they are a derivation of temperature extremes.\n",
    "\n",
    "```total_rain/snow```: Drop columns as all null.\n",
    "\n",
    "```total_precip```: Fill Nan with zeros as can assume no precipitaion.    \n",
    "\n",
    "```snow_on_ground```: Forward fill as the snow remains monstly constant for missing data points\n",
    "\n",
    "```dir_of_max_gust```: Drop as wind direction isn't as important as wind speed and wind directions of winds under 27km/h aren't recorded.\n",
    "\n",
    "```spd_of_max_gust```: Fill with zeroes, this is only reported if wind is greated than 31km/h. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 896 entries, 325 to 2139\n",
      "Data columns (total 17 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   btl_rating           896 non-null    object \n",
      " 1   tln_rating           896 non-null    object \n",
      " 2   alp_rating           896 non-null    object \n",
      " 3   problem_1            896 non-null    object \n",
      " 4   problem_2            896 non-null    object \n",
      " 5   problem_3            896 non-null    object \n",
      " 6   year                 896 non-null    int64  \n",
      " 7   month                896 non-null    int64  \n",
      " 8   day                  896 non-null    int64  \n",
      " 9   max_temp_c           896 non-null    float64\n",
      " 10  min_temp_c           896 non-null    float64\n",
      " 11  mean_temp_c          896 non-null    float64\n",
      " 12  heat_deg_day_c       896 non-null    float64\n",
      " 13  cool_deg_day_c       896 non-null    float64\n",
      " 14  total_precip_mm      896 non-null    float64\n",
      " 15  snow_on_ground_cm    896 non-null    float64\n",
      " 16  spd_of_max_gust_kmh  896 non-null    float64\n",
      "dtypes: float64(8), int64(3), object(6)\n",
      "memory usage: 126.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Implement the data cleaning for each column as outlined above\n",
    "import re\n",
    "pattern = r'[^\\w]'\n",
    "\n",
    "#Dropping unneeded columns\n",
    "df = df.drop(columns= ['date', 'chance_1', 'chance_2', 'chance_3', 'longitude', 'latitude', 'climate_id', 'station_name', 'data_quality', 'total_rain_mm', 'total_snow_cm', 'dir_of_max_gust_10s_deg'])\n",
    "\n",
    "#First 325 rows are missing avalanche rating data, drop them\n",
    "df.drop(axis = 0, inplace= True, index= range(0, 325))\n",
    "\n",
    "#Avalanche problems\n",
    "df['problem_1'].fillna('noproblem', inplace= True)\n",
    "df['problem_2'].fillna('noproblem', inplace= True)\n",
    "df['problem_3'].fillna('noproblem', inplace= True)\n",
    "df['problem_1'] = df['problem_1'].replace(pattern, '', regex= True)\n",
    "df['problem_2'] = df['problem_2'].replace(pattern, '', regex= True)\n",
    "df['problem_3'] = df['problem_3'].replace(pattern, '', regex= True)\n",
    "\n",
    "#Temperatures\n",
    "df['max_temp_c'].ffill(inplace= True)\n",
    "df['min_temp_c'].ffill(inplace= True)\n",
    "df['mean_temp_c'].ffill(inplace= True)\n",
    "df['cool_deg_day_c'].ffill(inplace= True)\n",
    "df['heat_deg_day_c'].ffill(inplace= True)\n",
    "\n",
    "#Precipitation\n",
    "df['total_precip_mm'].fillna(0, inplace= True)\n",
    "df['snow_on_ground_cm'].ffill(inplace= True)\n",
    "\n",
    "#Wind Speed\n",
    "df['spd_of_max_gust_kmh'].fillna(0, inplace= True)\n",
    "\n",
    "#Dropping of summer months\n",
    "df = df[df.month != 5]\n",
    "df = df[df.month != 6]\n",
    "df = df[df.month != 7]\n",
    "df = df[df.month != 8]\n",
    "df = df[df.month != 9]\n",
    "df = df[df.month != 10]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset now contains 1815 non-null rows. Inspecting the head of the data shows the new format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>btl_rating</th>\n",
       "      <th>tln_rating</th>\n",
       "      <th>alp_rating</th>\n",
       "      <th>problem_1</th>\n",
       "      <th>problem_2</th>\n",
       "      <th>problem_3</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>max_temp_c</th>\n",
       "      <th>min_temp_c</th>\n",
       "      <th>mean_temp_c</th>\n",
       "      <th>heat_deg_day_c</th>\n",
       "      <th>cool_deg_day_c</th>\n",
       "      <th>total_precip_mm</th>\n",
       "      <th>snow_on_ground_cm</th>\n",
       "      <th>spd_of_max_gust_kmh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>-8.6</td>\n",
       "      <td>-11.1</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>stormslab</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>-8.9</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>moderate</td>\n",
       "      <td>windslab</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>moderate</td>\n",
       "      <td>windslab</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>-5.3</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>-9.2</td>\n",
       "      <td>27.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>moderate</td>\n",
       "      <td>windslab</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>noproblem</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>-19.8</td>\n",
       "      <td>-14.5</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    btl_rating tln_rating alp_rating  problem_1  problem_2  problem_3  year  \\\n",
       "325        low        low        low  noproblem  noproblem  noproblem  2019   \n",
       "326        low        low        low  stormslab  noproblem  noproblem  2019   \n",
       "327        low        low   moderate   windslab  noproblem  noproblem  2019   \n",
       "328        low        low   moderate   windslab  noproblem  noproblem  2019   \n",
       "329        low        low   moderate   windslab  noproblem  noproblem  2019   \n",
       "\n",
       "     month  day  max_temp_c  min_temp_c  mean_temp_c  heat_deg_day_c  \\\n",
       "325     11   22        -8.6       -11.1         -9.8            27.8   \n",
       "326     11   23        -3.5        -8.9         -6.2            24.2   \n",
       "327     11   24        -1.3        -6.2         -3.8            21.8   \n",
       "328     11   25        -5.3       -13.0         -9.2            27.2   \n",
       "329     11   26        -9.1       -19.8        -14.5            32.5   \n",
       "\n",
       "     cool_deg_day_c  total_precip_mm  snow_on_ground_cm  spd_of_max_gust_kmh  \n",
       "325             0.0              0.0               43.0                 58.0  \n",
       "326             0.0              0.0               44.0                 48.0  \n",
       "327             0.0              0.0               57.0                 55.0  \n",
       "328             0.0              0.0               54.0                  0.0  \n",
       "329             0.0              0.0               52.0                  0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split\n",
    "With the pipline in place, we can split the data in target vectors and feature matrix. For this project we will be investingating the ratings for three different areas, thus we will have three target vectors. Each of these target vectors will be trained individually and then compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape:  (896, 14)\n",
      "Target y_btl shape:  (896,)\n",
      "Target y_tln shape:  (896,)\n",
      "Target y_alp shape:  (896,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Create target vectors and feature matrix\n",
    "X = df.drop(columns=['btl_rating', 'tln_rating', 'alp_rating'])\n",
    "y_btl = df['btl_rating']\n",
    "y_tln = df['tln_rating']\n",
    "y_alp = df['alp_rating']\n",
    "\n",
    "print('Feature matrix shape: ', X.shape)\n",
    "print('Target y_btl shape: ', y_btl.shape)\n",
    "print('Target y_tln shape: ', y_tln.shape)\n",
    "print('Target y_alp shape: ', y_alp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Creating a column transformer and pipeline will allow us to encode and normalize our dataset. Note, target vectors do not need to be encoded for sklearn classifiers so the rating columns can remain strings. Here we create a few options for our analysis:\n",
    "\n",
    "```configureParams(long, one_hot_columns)``` lets us choose if we want to use a long or short list of parameters (long takes about ~9 minutes to run, short takes ~2 minutes), and whether we include the ```problem``` columns from the dataset. The problems columns are field observations from avalanche forecasters, so by exluding these features we are testing how accurate our models are using ONLY weather data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Options:\n",
      "Use Long analysis - True\n",
      "Include problem fields in analysis - False\n",
      "\n",
      "### Below Treeline\n",
      "Best cross-validation training score: 0.9961379310344828\n",
      "Best cross-validation test score: 0.7012345679012346\n",
      "Test Set Accuracy 0.7111111111111111\n",
      "Best Classifier: RandomForestClassifier(random_state=0)\n",
      "Best Scaler: StandardScaler()\n",
      "Best Hyperparameters: {'classifier__max_depth': 15, 'classifier__n_estimators': 400}\n",
      "\n",
      "### Treeline\n",
      "Best cross-validation training score: 0.9997241379310345\n",
      "Best cross-validation test score: 0.6172839506172839\n",
      "Test Set Accuracy 0.6333333333333333\n",
      "Best Classifier: RandomForestClassifier(random_state=0)\n",
      "Best Scaler: StandardScaler()\n",
      "Best Hyperparameters: {'classifier__max_depth': 15, 'classifier__n_estimators': 1200}\n",
      "\n",
      "### Alpine\n",
      "Best cross-validation training score: 0.9997241379310345\n",
      "Best cross-validation test score: 0.6296296296296297\n",
      "Test Set Accuracy 0.5555555555555556\n",
      "Best Classifier: RandomForestClassifier(random_state=0)\n",
      "Best Scaler: StandardScaler()\n",
      "Best Hyperparameters: {'classifier__max_depth': 15, 'classifier__n_estimators': 1200}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import set_config\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Function for configuring how we conduct the analysis\n",
    "def configureParams(long, one_hot_columns):\n",
    "    ct1 = ColumnTransformer(transformers=[('scaling', StandardScaler(), make_column_selector(dtype_include= float)),\n",
    "                            ('onehotencoding', OneHotEncoder(sparse_output= True, handle_unknown='ignore'), one_hot_columns)\n",
    "                            ])\n",
    "    ct2 = ColumnTransformer(transformers=[('scaling', RobustScaler(), make_column_selector(dtype_include= float)),\n",
    "                            ('onehotencoding', OneHotEncoder(sparse_output= True, handle_unknown='ignore'), one_hot_columns)\n",
    "                            ])\n",
    "    \n",
    "    #Longer list of parameters\n",
    "    param_grid_long = [\n",
    "        {\n",
    "            'preprocessor': [ct1, ct2],\n",
    "            'classifier': [LogisticRegression(random_state=0)]\n",
    "            ,\n",
    "            'classifier__max_iter': [1000, 2000, 3000],\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__fit_intercept': [True, False]\n",
    "        }\n",
    "        ,\n",
    "        {\n",
    "            'preprocessor': [ct1, ct2],\n",
    "            'classifier': [RandomForestClassifier(random_state=0)]\n",
    "            ,\n",
    "            'classifier__n_estimators': [400, 800, 1200],\n",
    "            'classifier__max_depth': [11, 13, 15]\n",
    "        }\n",
    "        ,\n",
    "        {\n",
    "            'preprocessor': [ct1, ct2],\n",
    "            'classifier': [MLPClassifier(random_state=0)]\n",
    "            ,\n",
    "            'classifier__hidden_layer_sizes': [50, 100, 200],\n",
    "            'classifier__solver': ['adam', 'sgd', 'lbfgs']\n",
    "        }\n",
    "        ,\n",
    "        {\n",
    "            'preprocessor': [ct1, ct2],\n",
    "            'classifier': [SVC(random_state=0)]\n",
    "            ,\n",
    "            'classifier__C': [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "        }\n",
    "        ,\n",
    "\n",
    "    ]\n",
    "\n",
    "    #Shorter list of parameters\n",
    "    param_grid_short = [\n",
    "    {\n",
    "        'preprocessor': [ct1],\n",
    "        'classifier': [LogisticRegression(random_state=0)],\n",
    "        'classifier__max_iter': [2000],\n",
    "        'classifier__C': [0.001],\n",
    "        'classifier__fit_intercept': [True]\n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'preprocessor': [ct1],\n",
    "        'classifier': [RandomForestClassifier(random_state=0)],\n",
    "        'classifier__n_estimators': [50],\n",
    "        'classifier__max_depth': [5]\n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'preprocessor': [ct1],\n",
    "        'classifier': [MLPClassifier(random_state=0)],\n",
    "        'classifier__hidden_layer_sizes': [25],\n",
    "        'classifier__solver': ['adam']\n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'preprocessor': [ct1],\n",
    "        'classifier': [SVC(random_state=0)],\n",
    "        'classifier__C': [0.01]\n",
    "\n",
    "    }\n",
    "    ,\n",
    "\n",
    "]\n",
    "    return param_grid_long if long else param_grid_short\n",
    "\n",
    "#Function for creating a pipeline\n",
    "def configurePipeline(long, include_problem_fields, one_hot_columns):\n",
    "    \n",
    "    #Create column transformer.  handle_unknown is one way to deal with the fact that not all problems occur in all problem columns.\n",
    "    ct1 = ColumnTransformer(transformers=[('scaling', StandardScaler(), make_column_selector(dtype_include= float)),\n",
    "                            ('onehotencoding', OneHotEncoder(sparse_output= False, handle_unknown='ignore'), one_hot_columns)\n",
    "                            ])\n",
    "    ct2 = ColumnTransformer(transformers=[('scaling', RobustScaler(), make_column_selector(dtype_include= float)),\n",
    "                            ('onehotencoding', OneHotEncoder(sparse_output= False, handle_unknown='ignore'), one_hot_columns)\n",
    "                            ])\n",
    "\n",
    "    #Create pipeline\n",
    "    pipe = Pipeline(steps= [('preprocessor', ct1),\n",
    "                            ('classifier', LogisticRegression(max_iter=1000))])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch Cross Validation\n",
    "\n",
    "Perform the gridSearchCV using the pipeline functions defined above. Note, you can tune the parameters under the \"Analysis Options\" section (default is false, false). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################\n",
    "# ANALYSIS OPTIONS\n",
    "long = False\n",
    "include_problem_fields = False\n",
    "#####################\n",
    "\n",
    "print(\"### Options:\")\n",
    "print(\"Use Long analysis - \" + str(long))\n",
    "print(\"Include problem fields in analysis - \" + str(include_problem_fields))\n",
    "print(\"\")\n",
    "\n",
    "#Configuring onehot encoding columns, parameter grid, pipeline, and final X dataframe\n",
    "one_hot_columns = ['problem_1', 'problem_2', 'problem_3', 'year', 'month', 'day'] if include_problem_fields else ['year', 'month', 'day']\n",
    "param_grid = configureParams(long, one_hot_columns)\n",
    "pipe = configurePipeline(long, include_problem_fields, one_hot_columns)\n",
    "X_final = X if include_problem_fields else X.drop(['problem_1', 'problem_2', 'problem_3'], axis=1)\n",
    "\n",
    "#Visualize the pipeline\n",
    "set_config(display='diagram')\n",
    "\n",
    "#Create feature dictionary\n",
    "y_dict = {\"Below Treeline\": y_btl, \"Treeline\": y_tln, \"Alpine\": y_alp}\n",
    "\n",
    "#GridSearch function\n",
    "def performGridSearch(X, y, pipe, param_grid):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, stratify=y)\n",
    "    stratification = StratifiedShuffleSplit(n_splits=5)\n",
    "    # error_score='raise' will stop the fit if it hits an issue instead of putting nan in results\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=stratification, return_train_score=True, error_score='raise')\n",
    "    fit = grid_search.fit(X_train, y_train)\n",
    "    return grid_search, X_test, y_test\n",
    "\n",
    "#Perform an analysis for each of the target vectors (below treeline, treeline, alpine)\n",
    "for title, y in y_dict.items():\n",
    "\n",
    "    grid_search, X_test, y_test = performGridSearch(X_final, y, pipe, param_grid)\n",
    "    print(\"### \" + title)\n",
    "    print(\"Best cross-validation training score:\", grid_search.cv_results_['mean_train_score'][grid_search.best_index_])\n",
    "    print(\"Best cross-validation test score:\", grid_search.cv_results_['mean_test_score'][grid_search.best_index_])\n",
    "    print(\"Test Set Accuracy\", grid_search.best_estimator_.score(X_test, y_test))\n",
    "    print(\"Best Classifier:\", grid_search.best_params_['classifier'])\n",
    "    best_preprocessor = grid_search.best_estimator_['preprocessor']\n",
    "    print(\"Best Scaler:\", best_preprocessor.named_transformers_['scaling'])\n",
    "    model_hyperparameters = {key: value for key, value in grid_search.best_params_.items() if 'classifier__' in key}\n",
    "    print(\"Best Hyperparameters:\", model_hyperparameters)\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
